# üáÆüá© IndoBERT Sentiment Analysis (Version 1)

Fine-tuning **IndoBERT** (`indobenchmark/indobert-base-p1`) for **Indonesian sentiment classification** using tweet data.  
This repository contains two Google Colab‚Äìready notebooks:

| Notebook | Purpose |
|-----------|----------|
| [`notebooks/IndoBERT_V1.ipynb`](notebooks/IndoBERT_V1.ipynb) | End-to-end pipeline ‚Äî from preprocessing and label encoding to fine-tuning, evaluation, and artifact export. |
| [`notebooks/IndoBERT_Inference_V1.ipynb`](notebooks/IndoBERT_Inference_V1.ipynb) | Inference-only notebook ‚Äî load trained model, predict new texts, or run batch CSV predictions. |

---

## üß≠ Project Overview

**Objective:**  
Train a robust IndoBERT model to classify Indonesian texts (e.g., tweets) into `positif`, `netral`, or `negatif` sentiments.

**Workflow Summary**
1. **Preprocessing** ‚Äì load raw CSV (`tweet.csv`), clean whitespace, encode sentiment labels.  
2. **Dataset Split** ‚Äì 80 / 10 / 10 stratified split for train, validation, and test.  
3. **Tokenization** ‚Äì IndoBERT tokenizer with `max_length=128`.  
4. **Fine-Tuning** ‚Äì Transformers `Trainer`, optimizing for **macro-F1**.  
5. **Evaluation** ‚Äì accuracy, precision, recall, macro-F1, confusion matrix.  
6. **Export** ‚Äì best model checkpoint, tokenizer, label maps, metrics, and reports.  
7. **Inference** ‚Äì predict single texts or whole CSV files.

---

## üìÇ Folder Structure

```
Sentiment_IndoBERT/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ IndoBERT_V1.ipynb               # Full fine-tuning & evaluation
‚îÇ   ‚îî‚îÄ‚îÄ IndoBERT_Inference_V1.ipynb     # Inference-only version
‚îÇ
‚îú‚îÄ‚îÄ README.md                           # This file
‚îî‚îÄ‚îÄ (optional outputs on Drive)
    ‚îú‚îÄ‚îÄ best_model/                     # Saved fine-tuned model + tokenizer
    ‚îú‚îÄ‚îÄ label_maps.json
    ‚îú‚îÄ‚îÄ classification_report.txt
    ‚îú‚îÄ‚îÄ test_metrics.json
    ‚îú‚îÄ‚îÄ confusion_matrix_test.png
    ‚îî‚îÄ‚îÄ predictions_*.csv
```

---

## ‚öôÔ∏è Paths (Colab Defaults)

| Variable | Path |
|-----------|------|
| **DATA_PATH** | `/content/drive/MyDrive/Proyek/Sentiment_IndoBERT/Data/tweet.csv` |
| **OUTPUT_DIR** | `/content/drive/MyDrive/Proyek/Sentiment_IndoBERT` |
| **MODEL_DIR (for inference)** | `/content/drive/MyDrive/Proyek/Sentiment_IndoBERT/best_model` |

> Make sure your `tweet.csv` file has two columns:  
> `tweet` ‚Üí text input, and `sentimen` ‚Üí label (`positif`, `netral`, `negatif`).

---

## üöÄ How to Run (Training)

1. Open **[`notebooks/IndoBERT_V1.ipynb`](notebooks/IndoBERT_V1.ipynb)** in **Google Colab**.
2. Run all cells in order:
   - Mount Google Drive  
   - Confirm dataset path  
   - Train IndoBERT (‚âà 3 epochs by default)
3. Wait until the notebook prints final metrics:
   - **Accuracy**, **Precision**, **Recall**, **Macro-F1**
4. Check Drive folder for outputs:
   - `/best_model/`, `classification_report.txt`, `test_metrics.json`, `confusion_matrix_test.png`

Example classification report snippet:

```
              precision    recall  f1-score   support
    negatif      0.613     0.706     0.656       119
     netral      0.640     0.603     0.621       121
    positif     0.625     0.569     0.596       123
   macro avg    0.626     0.626     0.624       363
   accuracy                          0.625       363
```

---

## üîç How to Run (Inference Only)

1. Open **[`notebooks/IndoBERT_Inference_V1.ipynb`](notebooks/IndoBERT_Inference_V1.ipynb)**.
2. Make sure your fine-tuned model exists in:  
   `/content/drive/MyDrive/Proyek/Sentiment_IndoBERT/best_model`
3. Run **Process 1‚Äì3** to install dependencies, mount Drive, and load model.
4. Use the helper function:

```python
predict_texts([
    "Pelayanannya sangat memuaskan, terima kasih!",
    "Biasa saja sih, tidak terlalu istimewa.",
    "Sangat buruk, saya kecewa."
])
```

Expected output:

```python
['positif', 'netral', 'negatif']
```

5. To predict from a CSV:

```python
CSV_PATH = "/content/drive/MyDrive/Proyek/Sentiment_IndoBERT/Data/tweet.csv"
TEXT_COL = "tweet"
```

‚Üí Saves `predictions_YYYYMMDD_HHMMSS.csv` to your project folder.

---

## üß† Model & Training Configuration

| Parameter | Value |
|------------|--------|
| Model | `indobenchmark/indobert-base-p1` |
| Tokenizer | IndoBERT FastTokenizer |
| Epochs | 3 |
| Batch Size | 16 |
| Max Length | 128 |
| Learning Rate | 2e-5 |
| Optimizer | AdamW |
| Evaluation Metric | **Macro-F1** |
| Mixed Precision | FP16 / BF16 (auto-select) |

---

## üìä Outputs and Artifacts

- **best_model/** ‚Äì Saved weights + tokenizer  
- **label_maps.json** ‚Äì `label2id` & `id2label` mappings  
- **classification_report.txt** ‚Äì Precision / Recall / F1 per class  
- **test_metrics.json** ‚Äì Numeric metrics summary  
- **confusion_matrix_test.png** ‚Äì Visual evaluation  
- **predictions_*.csv** ‚Äì Batch inference results  

---

## üí° Tips

- To improve performance:
  - Increase `EPOCHS` to 4‚Äì5 and monitor overfitting.
  - Adjust `LR` between `2e-5` ‚Üí `3e-5`.
  - Use `MAX_LEN = 192` for longer texts.
- Use GPU runtime (`T4` / `A100`) on Colab.
- The Trainer automatically keeps the **best checkpoint** by macro-F1.

---

## üßæ Citation

If you use this project, please cite:

```
@misc{Sentiment_IndoBERT_2025,
  author = {moonandart (Aris)},
  title  = {IndoBERT Sentiment Analysis},
  year   = {2025},
  url    = {https://github.com/moonandart/Sentiment_IndoBERT}
}
```

---

## üß© Dependencies

- Python ‚â• 3.9  
- `transformers`  
- `datasets`  
- `accelerate`  
- `scikit-learn`  
- `matplotlib`  
- `sentencepiece`

Install (Colab):

```bash
!pip install -q transformers datasets accelerate scikit-learn matplotlib sentencepiece
```

---

## üèÅ Author

**Aris (moonandart)**  
AI & NLP enthusiast ‚Äî exploring sentiment analysis, BERT fine-tuning, and contextual AI tools.  
üîó [GitHub](https://github.com/moonandart)
